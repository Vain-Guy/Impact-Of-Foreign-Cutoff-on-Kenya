{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IMPORTING ALL RELEVANT LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibiqbvAoN_01"
      },
      "outputs": [],
      "source": [
        "# ------- [Import all relevant libraries] -------\n",
        "\n",
        "# Utilities\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Usual Suspects\n",
        "import numpy as np           # Mathematical operations\n",
        "import pandas as pd          # Data manipulation\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "import seaborn as sns\n",
        "\n",
        "# String manipulation\n",
        "import re\n",
        "\n",
        "# Pipelines\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# ML\n",
        "# Models\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LinearRegression, ElasticNet, ElasticNetCV, Ridge\n",
        "\n",
        "# ML Model Evaluation\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Time Series\n",
        "!pip install optuna\n",
        "import optuna\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "!pip install catboost\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LOADING THE DATA\n",
        "\n",
        "For this modeling section, we will use the data that has no feature engineering so we can focus on creating better features suited for modeling as opposed to EDA.\n",
        "\n",
        "Once loaded, we will do basic IDE(Initial Data Exploration) to make sure the data is consistent from when we saved it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mW8lHftbOgLk"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"../Clean Data/data_no_feature_engineering.csv\")\n",
        "\n",
        "# Create a copy\n",
        "df = data.copy(deep=True)\n",
        "\n",
        "# Check the shape\n",
        "print(f\"The dataset contains {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
        "\n",
        "# Check column names\n",
        "print(\"\\nColumn names:\")\n",
        "display(df.columns)\n",
        "\n",
        "# Check metadata\n",
        "print('\\n'+\"--\"*50+'\\n')\n",
        "display(df.info())\n",
        "\n",
        "# Check for duplicates and remove them\n",
        "print('\\n'+\"--\"*50+'\\n')\n",
        "print(\"Duplicates:\", df.duplicated().sum())\n",
        "\n",
        "# Drop duplicates\n",
        "df.drop_duplicates(inplace=True)\n",
        "print(\"Duplicates after removal:\", df.duplicated().sum())\n",
        "\n",
        "# Check for nulls\n",
        "print('\\n'+\"--\"*50+'\\n')\n",
        "print(\"Missing Values:\")\n",
        "display(df.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEnNxo4VTYF6"
      },
      "outputs": [],
      "source": [
        "# # Check for outliers\n",
        "# plt.figure(figsize=(7, 4))\n",
        "# sns.boxplot(data=df, palette=\"Set2\", linewidth=1.2)\n",
        "# plt.title(\"Outlier Summary\")\n",
        "# plt.xticks(rotation=0, ha=\"center\")\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# WRANGLING, BASIC EDA AND FEATURE ENGINEERING\n",
        "\n",
        "We remove redundant columns and do quick EDA to understand how best to handle feature engineering for our goals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VPOm3ttdE6i"
      },
      "outputs": [],
      "source": [
        "# Columns to drop\n",
        "cols_to_drop = [\n",
        "    'country_name',\n",
        "    'transaction_date',\n",
        "    'current_dollar_amount',\n",
        "    'implementing_partner_name',\n",
        "    'managing_subagency_or_bureau_name',\n",
        "    'implementing_partner_category_name',\n",
        "    'foreign_assistance_objective_name'\n",
        "]\n",
        "\n",
        "# Drop\n",
        "df = df.drop(columns=cols_to_drop)\n",
        "print(\"Remaining columns:\")\n",
        "display(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPWsM7v2Bb-6"
      },
      "outputs": [],
      "source": [
        "df['is_refund'] = (df['constant_dollar_amount'] < 0).astype(int)\n",
        "\n",
        "# Convert all dollar amounts to positive magnitudes\n",
        "df['constant_dollar_amount'] = df['constant_dollar_amount'].abs()\n",
        "\n",
        "print(\"Negative values remaining:\", (df['constant_dollar_amount'] < 0).sum())\n",
        "print(\"Refund transactions:\", df['is_refund'].sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Vu1o3nsJJ_W"
      },
      "outputs": [],
      "source": [
        "# --- Inspect managing agency counts ---\n",
        "agency_counts = df['managing_agency_name'].value_counts()\n",
        "display(agency_counts)\n",
        "\n",
        "# --- Plot distribution ---\n",
        "plt.figure(figsize=(12,7.5))\n",
        "sns.barplot(x=agency_counts.index, y=agency_counts.values, palette=\"magma\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Distribution of Projects by Managing Agency\")\n",
        "plt.ylabel(\"Number of Projects\")\n",
        "plt.xlabel(\"Managing Agency\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJCeQ77NMmEY"
      },
      "outputs": [],
      "source": [
        "# Inspect funding agency name\n",
        "agency_counts = df['funding_agency_name'].value_counts()\n",
        "display(agency_counts)\n",
        "\n",
        "# --- Plot distribution ---\n",
        "plt.figure(figsize=(10,7.5))\n",
        "sns.barplot(x=agency_counts.index, y=agency_counts.values, palette=\"viridis\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Distribution of Funding Agencies\")\n",
        "plt.ylabel(\"Number of Projects\")\n",
        "plt.xlabel(\"Funding Agency\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZZnHsHJJVO7"
      },
      "outputs": [],
      "source": [
        "# inspect us category name\n",
        "df['us_category_name'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAsrzz-2JkbO"
      },
      "outputs": [],
      "source": [
        "# Inspect us_sector_name\n",
        "df['us_sector_name'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3MPdmqGKLbh"
      },
      "outputs": [],
      "source": [
        "sector_mapping = {\n",
        "    # --- Health ---\n",
        "    'HIV/AIDS': 'Health',\n",
        "    'Malaria': 'Health',\n",
        "    'Tuberculosis': 'Health',\n",
        "    'Maternal and Child Health': 'Health',\n",
        "    'Family Planning and Reproductive Health': 'Health',\n",
        "    'Health - General': 'Health',\n",
        "    'Other Public Health Threats': 'Health',\n",
        "    'Pandemic Influenza and Other Emerging Threats (PIOET)': 'Health',\n",
        "    'Nutrition': 'Health',\n",
        "\n",
        "    # --- Education ---\n",
        "    'Basic Education': 'Education',\n",
        "    'Higher Education': 'Education',\n",
        "    'Education and Social Services - General': 'Education',\n",
        "\n",
        "    # --- Governance & Human Rights ---\n",
        "    'Rule of Law and Human Rights': 'Governance & Human Rights',\n",
        "    'Good Governance': 'Governance & Human Rights',\n",
        "    'Democracy, Human Rights, and Governance - General': 'Governance & Human Rights',\n",
        "    'Civil Society': 'Governance & Human Rights',\n",
        "    'Political Competition and Consensus-Building': 'Governance & Human Rights',\n",
        "    'Monitoring and Evaluation': 'Governance & Human Rights',\n",
        "\n",
        "    # --- Agriculture & Food Security ---\n",
        "    'Agriculture': 'Agriculture & Food Security',\n",
        "\n",
        "    # --- Economic Growth & Development ---\n",
        "    'Economic Opportunity': 'Economic Growth & Development',\n",
        "    'Economic Development - General': 'Economic Growth & Development',\n",
        "    'Financial Sector': 'Economic Growth & Development',\n",
        "    'Private Sector Competitiveness': 'Economic Growth & Development',\n",
        "    'Trade and Investment': 'Economic Growth & Development',\n",
        "    'Macroeconomic Foundation for Growth': 'Economic Growth & Development',\n",
        "    'Policies, Regulations, and Systems': 'Economic Growth & Development',\n",
        "\n",
        "    # --- Infrastructure & Environment ---\n",
        "    'Infrastructure': 'Infrastructure & Environment',\n",
        "    'Water Supply and Sanitation': 'Infrastructure & Environment',\n",
        "    'Clean Productive Environment': 'Infrastructure & Environment',\n",
        "    'Environment - General': 'Infrastructure & Environment',\n",
        "    'Natural Resources and Biodiversity': 'Infrastructure & Environment',\n",
        "    'Manufacturing': 'Infrastructure & Environment',\n",
        "    'Mining and Natural Resources': 'Infrastructure & Environment',\n",
        "    'Environment': 'Infrastructure & Environment',\n",
        "\n",
        "    # --- Peace & Security ---\n",
        "    'Stabilization Operations and Security Sector Reform': 'Peace & Security',\n",
        "    'Counter-Terrorism': 'Peace & Security',\n",
        "    'Counter-Narcotics': 'Peace & Security',\n",
        "    'Conflict Mitigation and Reconciliation': 'Peace & Security',\n",
        "    'Peace and Security - General': 'Peace & Security',\n",
        "    'Transnational Crime': 'Peace & Security',\n",
        "    'Combating Weapons of Mass Destruction (WMD)': 'Peace & Security',\n",
        "\n",
        "    # --- Humanitarian & Social Protection ---\n",
        "    'Protection, Assistance and Solutions': 'Humanitarian & Social Protection',\n",
        "    'Disaster Readiness': 'Humanitarian & Social Protection',\n",
        "    'Migration Management': 'Humanitarian & Social Protection',\n",
        "    'Humanitarian Assistance - General': 'Humanitarian & Social Protection',\n",
        "    'Social Services': 'Humanitarian & Social Protection',\n",
        "    'Social Assistance': 'Humanitarian & Social Protection',\n",
        "\n",
        "    # --- Governance & Administration ---\n",
        "    'Direct Administrative Costs': 'Governance & Administration',\n",
        "    'Multi-sector - Unspecified': 'Governance & Administration',\n",
        "    'International Contributions': 'Governance & Administration',\n",
        "    'Labor Policies and Markets': 'Governance & Administration',\n",
        "}\n",
        "\n",
        "# Apply mapping\n",
        "df['sector'] = df['us_sector_name'].map(sector_mapping)\n",
        "df.drop(columns='us_sector_name', inplace=True)\n",
        "\n",
        "# --- Check distribution ---\n",
        "sector_counts = df['sector'].value_counts(dropna=False)\n",
        "display(sector_counts)\n",
        "\n",
        "# --- Plot distribution ---\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=sector_counts.index, y=sector_counts.values, palette=\"coolwarm\")\n",
        "plt.xticks(rotation=90, ha='center')\n",
        "plt.title(\"Distribution of Projects by Sector\")\n",
        "plt.ylabel(\"Number of Projects\")\n",
        "plt.xlabel(\"Sector\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFgn0NHXMwuo"
      },
      "outputs": [],
      "source": [
        "# --- Count projects per fiscal year ---\n",
        "fy_counts = df['fiscal_year'].value_counts().sort_index()  # chronological order\n",
        "print(fy_counts)\n",
        "\n",
        "# --- Line plot ---\n",
        "plt.figure(figsize=(10,4))\n",
        "sns.lineplot(x=fy_counts.index, y=fy_counts.values, marker='o', color='teal')\n",
        "plt.title(\"Trend of Projects by Fiscal Year\")\n",
        "plt.xlabel(\"Fiscal Year\")\n",
        "plt.ylabel(\"Number of Projects\")\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djXL1KwLK84v"
      },
      "outputs": [],
      "source": [
        "# Drop us_category_name\n",
        "df.drop(columns='us_category_name', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UyVS-c0BuTJ"
      },
      "outputs": [],
      "source": [
        "# Sort for time series\n",
        "df = df.sort_values(['managing_agency_name', 'sector', 'fiscal_year']).reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FEATURE ENGINEERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veSz1-6wRQWq"
      },
      "outputs": [],
      "source": [
        "# --- FEATURE ENGINEERING ---\n",
        "group_cols = ['managing_agency_name', 'sector']\n",
        "target_col = 'constant_dollar_amount'\n",
        "\n",
        "# Define lag periods and rolling windows\n",
        "lags = [1, 2]\n",
        "rolling_windows = [3]\n",
        "\n",
        "# --- Lag features ---\n",
        "for lag in lags:\n",
        "    df[f'lag_{lag}'] = df.groupby(group_cols)[target_col].shift(lag)\n",
        "\n",
        "# --- Rolling mean and std ---\n",
        "for window in rolling_windows:\n",
        "    df[f'rolling_mean_{window}yr'] = df.groupby(group_cols)[target_col].transform(\n",
        "        lambda x: x.shift(1).rolling(window, min_periods=1).mean()\n",
        "    )\n",
        "    df[f'rolling_std_{window}yr'] = df.groupby(group_cols)[target_col].transform(\n",
        "        lambda x: x.shift(1).rolling(window, min_periods=1).std()\n",
        "    )\n",
        "\n",
        "# --- Growth rate ---\n",
        "df['funding_growth_rate'] = df.groupby(group_cols)[target_col].pct_change()\n",
        "\n",
        "# --- Fill NaNs with 0 ---\n",
        "feat_cols = [f'lag_{lag}' for lag in lags] + \\\n",
        "            [f'rolling_mean_{w}yr' for w in rolling_windows] + \\\n",
        "            [f'rolling_std_{w}yr' for w in rolling_windows] + \\\n",
        "            ['funding_growth_rate']\n",
        "\n",
        "df[feat_cols] = df[feat_cols].fillna(0)\n",
        "\n",
        "# Preview\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MODELING\n",
        "\n",
        "## VANILLA MODELING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofjke3R3V8c2"
      },
      "outputs": [],
      "source": [
        "# --- ML PIPELINES FOR RANDOM FOREST, XGBOOST, LIGHTGBM AND CATBOOST ---\n",
        "\n",
        "# --- 1. Separate features and target ---\n",
        "X = df.drop('constant_dollar_amount', axis=1)\n",
        "y = np.log1p(df['constant_dollar_amount'])  \n",
        "\n",
        "# Train-test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify categorical and numeric features\n",
        "cat_cols = x_train.select_dtypes(include=['object']).columns.tolist()\n",
        "num_cols = [c for c in x_train.columns if c not in cat_cols]\n",
        "\n",
        "# --- 2. Column transformer for preprocessing ---\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols),\n",
        "        ('num', 'passthrough', num_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# --- 3. Define pipelines for each model ---\n",
        "pipelines = {\n",
        "    'RandomForest': Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', RandomForestRegressor(random_state=42))\n",
        "    ]),\n",
        "    'XGBoost': Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', xgb.XGBRegressor(random_state=42, objective='reg:squarederror'))\n",
        "    ]),\n",
        "    'LightGBM': Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', lgb.LGBMRegressor(random_state=42))\n",
        "    ]),\n",
        "    'Catboost': Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', CatBoostRegressor(random_state=42, verbose=0))\n",
        "    ])\n",
        "}\n",
        "\n",
        "# --- 4. Train and evaluate ---\n",
        "for name, pipe in pipelines.items():\n",
        "    pipe.fit(x_train, y_train)\n",
        "    y_pred_log = pipe.predict(x_test)\n",
        "    y_pred = np.expm1(y_pred_log)  \n",
        "    y_true = np.expm1(y_test)\n",
        "\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"--- {name} ---\")\n",
        "    print(f\"MAE: {mae:,.2f}\")\n",
        "    print(f\"RMSE: {rmse:,.2f}\")\n",
        "    print(f\"R²: {r2:.4f}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ENSEMBLE STACKING OF BEST MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Base Models ---\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "cat_model = CatBoostRegressor(random_state=42, verbose=0)\n",
        "\n",
        "# --- 5. Meta Model ---\n",
        "meta_model = Ridge()\n",
        "\n",
        "# --- 6. Stacking Regressor ---\n",
        "stack_model = StackingRegressor(\n",
        "    estimators=[\n",
        "        ('xgb', xgb_model),\n",
        "        ('cat', cat_model)\n",
        "    ],\n",
        "    final_estimator=meta_model,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# --- 7. Pipeline ---\n",
        "stack_pipe = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', stack_model)\n",
        "])\n",
        "\n",
        "# --- 8. Fit and Evaluate ---\n",
        "print(\"Training vanilla stacking model...\\n\")\n",
        "stack_pipe.fit(x_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_log = stack_pipe.predict(x_test)\n",
        "y_pred = np.expm1(y_pred_log)\n",
        "y_true = np.expm1(y_test)\n",
        "\n",
        "# Metrics\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "# --- 9. Report ---\n",
        "print(\"=== VANILLA STACKING ENSEMBLE (XGB + CatBoost) ===\")\n",
        "print(f\"MAE: {mae:,.2f}\")\n",
        "print(f\"RMSE: {rmse:,.2f}\")\n",
        "print(f\"R²: {r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7TgyYQqCKwA"
      },
      "outputs": [],
      "source": [
        "# === VANILLA STACKING REGRESSION (XGB +  + RIDGE) ===\n",
        "\n",
        "# --- 1. Split features and target ---\n",
        "X = df.drop('constant_dollar_amount', axis=1)\n",
        "y = np.log1p(df['constant_dollar_amount'])  # Log-transform for stability\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, shuffle=False\n",
        ")\n",
        "\n",
        "# --- 2. Identify categorical and numeric columns ---\n",
        "cat_cols = x_train.select_dtypes(include=['object']).columns.tolist()\n",
        "num_cols = [c for c in x_train.columns if c not in cat_cols]\n",
        "\n",
        "# --- 3. Preprocessor ---\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False), cat_cols),\n",
        "        ('num', StandardScaler(), num_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# --- 4. Base Models ---\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
        "\n",
        "# --- 5. Meta Model ---\n",
        "meta_model = Ridge()\n",
        "\n",
        "# --- 6. Stacking Regressor ---\n",
        "stack_model = StackingRegressor(\n",
        "    estimators=[\n",
        "        ('xgb', xgb_model),\n",
        "        ('lgb', lgb_model)\n",
        "    ],\n",
        "    final_estimator=meta_model,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# --- 7. Pipeline ---\n",
        "stack_pipe = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', stack_model)\n",
        "])\n",
        "\n",
        "# --- 8. Fit and Evaluate ---\n",
        "print(\"Training vanilla stacking model...\\n\")\n",
        "stack_pipe.fit(x_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_log = stack_pipe.predict(x_test)\n",
        "y_pred = np.expm1(y_pred_log)\n",
        "y_true = np.expm1(y_test)\n",
        "\n",
        "# Metrics\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "# --- 9. Report ---\n",
        "print(\"=== VANILLA STACKING ENSEMBLE ===\")\n",
        "print(f\"MAE: {mae:,.2f}\")\n",
        "print(f\"RMSE: {rmse:,.2f}\")\n",
        "print(f\"R²: {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TUBED STACKING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3HLg5oaQ_B4"
      },
      "outputs": [],
      "source": [
        "# === TUNED STACKING REGRESSION (XGB + LGB + RIDGE with RandomizedSearchCV) ===\n",
        "\n",
        "# --- 1. Split features and target ---\n",
        "X = df.drop('constant_dollar_amount', axis=1)\n",
        "y = np.log1p(df['constant_dollar_amount'])\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, shuffle=False\n",
        ")\n",
        "\n",
        "# --- 2. Identify categorical and numeric columns ---\n",
        "cat_cols = x_train.select_dtypes(include=['object']).columns.tolist()\n",
        "num_cols = [c for c in x_train.columns if c not in cat_cols]\n",
        "\n",
        "# --- 3. Preprocessor ---\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False), cat_cols),\n",
        "        ('num', StandardScaler(), num_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# --- 4. Base Models ---\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
        "meta_model = Ridge()\n",
        "\n",
        "stack_model = StackingRegressor(\n",
        "    estimators=[\n",
        "        ('xgb', xgb_model),\n",
        "        ('lgb', lgb_model)\n",
        "    ],\n",
        "    final_estimator=meta_model,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "stack_pipe = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', stack_model)\n",
        "])\n",
        "\n",
        "# --- 5. Parameter Grid for RandomizedSearchCV ---\n",
        "param_grid = {\n",
        "    # XGBoost\n",
        "    'model__xgb__n_estimators': [100, 200, 300],\n",
        "    'model__xgb__max_depth': [3, 4, 5, 6, 8],\n",
        "    'model__xgb__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'model__xgb__subsample': [0.6, 0.8, 1.0],\n",
        "    'model__xgb__colsample_bytree': [0.6, 0.8, 1.0],\n",
        "\n",
        "    # LightGBM\n",
        "    'model__lgb__n_estimators': [100, 200, 300],\n",
        "    'model__lgb__num_leaves': [20, 31, 50, 70],\n",
        "    'model__lgb__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'model__lgb__subsample': [0.6, 0.8, 1.0],\n",
        "    'model__lgb__colsample_bytree': [0.6, 0.8, 1.0],\n",
        "\n",
        "    # Ridge meta-model\n",
        "    'model__final_estimator__alpha': np.logspace(-3, 2, 20)\n",
        "}\n",
        "\n",
        "# --- 6. Randomized Search ---\n",
        "random_search = RandomizedSearchCV(\n",
        "    stack_pipe,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=25,\n",
        "    cv=5,\n",
        "    scoring='r2',\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Running Randomized Search for Stacking Ensemble...\\n\")\n",
        "random_search.fit(x_train, y_train)\n",
        "\n",
        "# --- 7. Evaluation ---\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "y_pred_log = best_model.predict(x_test)\n",
        "y_pred = np.expm1(y_pred_log)\n",
        "y_true = np.expm1(y_test)\n",
        "\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "print(\"\\n=== TUNED STACKING ENSEMBLE RESULTS ===\")\n",
        "print(f\"Best Params: {random_search.best_params_}\")\n",
        "print(f\"MAE: {mae:,.2f}\")\n",
        "print(f\"RMSE: {rmse:,.2f}\")\n",
        "print(f\"R²: {r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0mZImyWPuyK"
      },
      "outputs": [],
      "source": [
        "# === TIME SERIES CROSS-VALIDATION + RANDOMIZED SEARCH FOR XGB ===\n",
        "\n",
        "# ---\n",
        "# We use time series cross validation for evaluating time series data\n",
        "# ---\n",
        "\n",
        "# --- 1. TimeSeriesSplit setup ---\n",
        "tscv = TimeSeriesSplit(n_splits=8)\n",
        "\n",
        "# --- 2. Model parameter grids ---\n",
        "\n",
        "# XGBoost\n",
        "xgb_params = {\n",
        "    'model__n_estimators': [100, 300],\n",
        "    'model__learning_rate': [0.01, 0.05, 0.1],\n",
        "    'model__max_depth': [4, 6, 8],\n",
        "    'model__subsample': [0.7, 0.9, 1.0],\n",
        "    'model__colsample_bytree': [0.7, 0.9, 1.0],\n",
        "    'model__reg_lambda': [1, 2, 5]\n",
        "}\n",
        "\n",
        "# --- 3. Model pipelines ---\n",
        "pipelines = {\n",
        "    'XGBoost': Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', xgb.XGBRegressor(random_state=42, objective='reg:squarederror'))\n",
        "    ])\n",
        "}\n",
        "\n",
        "# --- 4. Randomized Search ---\n",
        "param_grids = {'XGBoost': xgb_params}\n",
        "\n",
        "best_models = {}\n",
        "for name, pipe in pipelines.items():\n",
        "    print(f\"\\nTuning {name} with TimeSeriesSplit...\\n\")\n",
        "\n",
        "    search = RandomizedSearchCV(\n",
        "        estimator=pipe,\n",
        "        param_distributions=param_grids[name],\n",
        "        n_iter=20,               # test 20 random combinations\n",
        "        cv=tscv,\n",
        "        scoring='r2',\n",
        "        verbose=2,\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    search.fit(X, y)\n",
        "    best_models[name] = search.best_estimator_\n",
        "\n",
        "    print(f\"Best R² (CV): {search.best_score_:.4f}\")\n",
        "    print(f\"Best Params: {search.best_params_}\\n\")\n",
        "\n",
        "# --- 5. Evaluate on final holdout (last few years, e.g. 2020–2025) ---\n",
        "holdout = df[df['fiscal_year'] >= 2020]\n",
        "train = df[df['fiscal_year'] < 2020]\n",
        "\n",
        "X_train = train.drop('constant_dollar_amount', axis=1)\n",
        "y_train = np.log1p(train['constant_dollar_amount'])\n",
        "X_test = holdout.drop('constant_dollar_amount', axis=1)\n",
        "y_test = np.log1p(holdout['constant_dollar_amount'])\n",
        "\n",
        "for name, model in best_models.items():\n",
        "    y_pred_log = model.predict(X_test)\n",
        "    y_pred = np.expm1(y_pred_log)\n",
        "    y_true = np.expm1(y_test)\n",
        "\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"--- {name} (Final Holdout Evaluation) ---\")\n",
        "    print(f\"MAE: {mae:,.2f}\")\n",
        "    print(f\"RMSE: {rmse:,.2f}\")\n",
        "    print(f\"R²: {r2:.4f}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UpdIiTV2pm2"
      },
      "outputs": [],
      "source": [
        "# === OPTUNA OPTIMIZATION FOR XGBOOST WITH TIME SERIES SPLIT ===\n",
        "\n",
        "# --- 1. TimeSeriesSplit setup ---\n",
        "tscv = TimeSeriesSplit(n_splits=8)\n",
        "\n",
        "# --- 2. Define Optuna objective ---\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0, log=True),\n",
        "        'random_state': 42,\n",
        "        'objective': 'reg:squarederror',\n",
        "        'n_jobs': -1\n",
        "    }\n",
        "\n",
        "    pipe = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', xgb.XGBRegressor(**params))\n",
        "    ])\n",
        "\n",
        "    scores = cross_val_score(pipe, X, y, cv=tscv, scoring='r2', n_jobs=-1)\n",
        "    return np.mean(scores)\n",
        "\n",
        "# --- 3. Run Optuna study ---\n",
        "print(\"=== OPTIMIZING XGBOOST (Optuna) ===\")\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
        "\n",
        "print(f\"\\nBest R² (CV): {study.best_value:.4f}\")\n",
        "print(\"Best Params:\", study.best_params)\n",
        "\n",
        "# --- 4. Fit best model ---\n",
        "best_xgb = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', xgb.XGBRegressor(**study.best_params))\n",
        "]).fit(X, y)\n",
        "\n",
        "# --- 5. Final holdout evaluation (2020–2025) ---\n",
        "holdout = df[df['fiscal_year'] >= 2020]\n",
        "train = df[df['fiscal_year'] < 2020]\n",
        "\n",
        "X_train = train.drop('constant_dollar_amount', axis=1)\n",
        "y_train = np.log1p(train['constant_dollar_amount'])\n",
        "X_test = holdout.drop('constant_dollar_amount', axis=1)\n",
        "y_test = np.log1p(holdout['constant_dollar_amount'])\n",
        "\n",
        "# Predictions\n",
        "y_pred_log = best_xgb.predict(X_test)\n",
        "y_pred = np.expm1(y_pred_log)\n",
        "y_true = np.expm1(y_test)\n",
        "\n",
        "# Metrics\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "print(f\"\\n--- XGBoost (Final Holdout Evaluation) ---\")\n",
        "print(f\"MAE: {mae:,.2f}\")\n",
        "print(f\"RMSE: {rmse:,.2f}\")\n",
        "print(f\"R²: {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHw5zFVtCJIf"
      },
      "outputs": [],
      "source": [
        "# ---\n",
        "# Save our best model- XGBoost with RandomizedSearchCV optimization\n",
        "# ---\n",
        "\n",
        "import joblib\n",
        "\n",
        "joblib.dump(best_models['XGBoost'], 'xgb_best_pipeline.pkl')\n",
        "\n",
        "print(\"XGBoost pipeline saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsTxD9AnDKRy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
